{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef6445a6",
   "metadata": {},
   "source": [
    "# Data Ingestion and Raw Tables\n",
    "\n",
    "This notebook performs the initial data ingestion from source files and creates raw tables in the data warehouse. The process includes:\n",
    "\n",
    "1. Loading source data from JSON and CSV files\n",
    "2. Basic data validation and cleanup\n",
    "3. Saving data in Parquet format for efficient processing\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, we'll import required libraries and initialize our Spark session:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b32651",
   "metadata": {},
   "source": [
    "# E-commerce Data Ingestion and Raw Tables\n",
    "\n",
    "This notebook handles the initial data ingestion from various sources (CSV, JSON, Excel) and creates raw tables for further processing.\n",
    "\n",
    "## Contents\n",
    "1. Setup and Dependencies\n",
    "2. Schema Definitions\n",
    "3. Data Loading\n",
    "4. Initial Validation\n",
    "5. Raw Table Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47b3f02",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Import required libraries\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StructType, StructField, StringType, IntegerType, FloatType, DateType\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DateType\n",
    "from datetime import datetime\n",
    "from src.config import SparkConfig, DataConfig, get_spark_configs\n",
    "\n",
    "# Initialize Spark session with configurations\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(SparkConfig.APP_NAME) \\\n",
    "    .master(SparkConfig.MASTER)\n",
    "\n",
    "# Add all configurations\n",
    "for key, value in get_spark_configs().items():\n",
    "    spark = spark.config(key, value)\n",
    "\n",
    "spark = spark.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40f26c8",
   "metadata": {},
   "source": [
    "## Load Raw Data\n",
    "\n",
    "Next, we'll load the raw data from JSON and CSV files. We'll display a sample of each dataset to verify the data loading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4e3db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data directory: c:\\Users\\kulde\\Downloads\\PEI\\ecom_assignment\\data\n"
     ]
    }
   ],
   "source": [
    "# Get project root directory\n",
    "import os\n",
    "from src.config import DataConfig\n",
    "\n",
    "# Set up paths\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "data_dir = os.path.join(project_root, DataConfig.DATA_DIR)\n",
    "processed_dir = os.path.join(project_root, DataConfig.PROCESSED_DIR)\n",
    "\n",
    "# Verify directories exist\n",
    "if not os.path.exists(data_dir):\n",
    "    raise FileNotFoundError(f\"Data directory not found at {data_dir}\")\n",
    "\n",
    "# Create processed directory if it doesn't exist\n",
    "os.makedirs(processed_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Using data directory: {data_dir}\")\n",
    "print(f\"Using processed directory: {processed_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac837e4",
   "metadata": {},
   "source": [
    "## Schema Definitions\n",
    "\n",
    "Define the schemas for our raw tables to ensure data consistency and proper type enforcement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f44add59",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'StructType' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Define schema for Products table\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m products_schema = \u001b[43mStructType\u001b[49m([\n\u001b[32m      3\u001b[39m     StructField(\u001b[33m\"\u001b[39m\u001b[33mProduct ID\u001b[39m\u001b[33m\"\u001b[39m, StringType(), \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[32m      4\u001b[39m     StructField(\u001b[33m\"\u001b[39m\u001b[33mCategory\u001b[39m\u001b[33m\"\u001b[39m, StringType(), \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[32m      5\u001b[39m     StructField(\u001b[33m\"\u001b[39m\u001b[33mSub-Category\u001b[39m\u001b[33m\"\u001b[39m, StringType(), \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[32m      6\u001b[39m     StructField(\u001b[33m\"\u001b[39m\u001b[33mProduct Name\u001b[39m\u001b[33m\"\u001b[39m, StringType(), \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[32m      7\u001b[39m     StructField(\u001b[33m\"\u001b[39m\u001b[33mPrice\u001b[39m\u001b[33m\"\u001b[39m, FloatType(), \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      8\u001b[39m ])\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Define schema for Customers table\u001b[39;00m\n\u001b[32m     11\u001b[39m customers_schema = StructType([\n\u001b[32m     12\u001b[39m     StructField(\u001b[33m\"\u001b[39m\u001b[33mCustomer ID\u001b[39m\u001b[33m\"\u001b[39m, StringType(), \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[32m     13\u001b[39m     StructField(\u001b[33m\"\u001b[39m\u001b[33mCustomer Name\u001b[39m\u001b[33m\"\u001b[39m, StringType(), \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[32m   (...)\u001b[39m\u001b[32m     17\u001b[39m     StructField(\u001b[33m\"\u001b[39m\u001b[33mPostal Code\u001b[39m\u001b[33m\"\u001b[39m, StringType(), \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     18\u001b[39m ])\n",
      "\u001b[31mNameError\u001b[39m: name 'StructType' is not defined"
     ]
    }
   ],
   "source": [
    "# Define schema for Products table\n",
    "products_schema = StructType([\n",
    "    StructField(\"Product ID\", StringType(), False),\n",
    "    StructField(\"Category\", StringType(), True),\n",
    "    StructField(\"Sub-Category\", StringType(), True),\n",
    "    StructField(\"Product Name\", StringType(), False),\n",
    "    StructField(\"Price\", FloatType(), True)\n",
    "])\n",
    "\n",
    "# Define schema for Customers table\n",
    "customers_schema = StructType([\n",
    "    StructField(\"Customer ID\", StringType(), False),\n",
    "    StructField(\"Customer Name\", StringType(), False),\n",
    "    StructField(\"Email\", StringType(), True),\n",
    "    StructField(\"Country\", StringType(), True),\n",
    "    StructField(\"City\", StringType(), True),\n",
    "    StructField(\"Postal Code\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Define schema for Orders table\n",
    "orders_schema = StructType([\n",
    "    StructField(\"Order ID\", StringType(), False),\n",
    "    StructField(\"Customer ID\", StringType(), False),\n",
    "    StructField(\"Product ID\", StringType(), False),\n",
    "    StructField(\"Order Date\", DateType(), False),\n",
    "    StructField(\"Quantity\", IntegerType(), False),\n",
    "    StructField(\"Sales\", FloatType(), False),\n",
    "    StructField(\"Profit\", FloatType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4e8d24",
   "metadata": {},
   "source": [
    "## Data Validation Functions\n",
    "\n",
    "Define functions to validate data quality according to configured thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f695b5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_dataframe(df, df_name):\n",
    "    \"\"\"Validate DataFrame against quality thresholds\"\"\"\n",
    "    from pyspark.sql.functions import col, count, when\n",
    "    \n",
    "    # Check required columns\n",
    "    missing_cols = set(DataConfig.REQUIRED_COLUMNS[df_name]) - set(df.columns)\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Missing required columns in {df_name}: {missing_cols}\")\n",
    "    \n",
    "    # Check for nulls\n",
    "    for column in DataConfig.REQUIRED_COLUMNS[df_name]:\n",
    "        null_count = df.filter(col(column).isNull()).count()\n",
    "        null_percentage = null_count / df.count()\n",
    "        \n",
    "        if null_percentage > DataConfig.MAX_NULL_PERCENTAGE:\n",
    "            print(f\"WARNING: Column {column} has {null_percentage:.2%} null values\")\n",
    "    \n",
    "    # Check for duplicates\n",
    "    if df_name == \"orders\":  # Only check duplicates for order records\n",
    "        duplicate_count = df.count() - df.dropDuplicates([\"Order ID\"]).count()\n",
    "        duplicate_percentage = duplicate_count / df.count()\n",
    "        \n",
    "        if duplicate_percentage > DataConfig.MAX_DUPLICATE_PERCENTAGE:\n",
    "            print(f\"WARNING: Found {duplicate_percentage:.2%} duplicate orders\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89893d3",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "Load data from various source files (CSV, JSON, Excel) and apply the defined schemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20aae8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Products data from CSV\n",
    "products_df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(products_schema) \\\n",
    "    .load(os.path.join(data_dir, \"Products.csv\"))\n",
    "\n",
    "# Load Customers data from Excel\n",
    "customers_df = spark.createDataFrame(\n",
    "    pd.read_excel(os.path.join(data_dir, \"Customer.xlsx\")),\n",
    "    schema=customers_schema\n",
    ")\n",
    "\n",
    "# Load Orders data from JSON\n",
    "orders_df = spark.read.format(\"json\") \\\n",
    "    .schema(orders_schema) \\\n",
    "    .load(os.path.join(data_dir, \"Orders.json\"))\n",
    "\n",
    "# Show sample data\n",
    "print(\"Products sample:\")\n",
    "products_df.show(5)\n",
    "print(\"\\nCustomers sample:\")\n",
    "customers_df.show(5)\n",
    "print(\"\\nOrders sample:\")\n",
    "orders_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c6cc4d",
   "metadata": {},
   "source": [
    "## Initial Data Validation\n",
    "\n",
    "Perform basic data quality checks on the loaded data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cb454e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "def check_missing_values(df, table_name):\n",
    "    print(f\"\\nMissing values in {table_name}:\")\n",
    "    for col in df.columns:\n",
    "        missing_count = df.filter(df[col].isNull()).count()\n",
    "        if missing_count > 0:\n",
    "            print(f\"{col}: {missing_count} missing values\")\n",
    "\n",
    "# Check for duplicate keys\n",
    "def check_duplicate_keys(df, key_col, table_name):\n",
    "    duplicate_count = df.groupBy(key_col).count().filter(\"count > 1\").count()\n",
    "    print(f\"\\nDuplicate keys in {table_name}: {duplicate_count}\")\n",
    "\n",
    "# Perform checks\n",
    "for df, name, key in [\n",
    "    (products_df, \"Products\", \"Product ID\"),\n",
    "    (customers_df, \"Customers\", \"Customer ID\"),\n",
    "    (orders_df, \"Orders\", \"Order ID\")\n",
    "]:\n",
    "    check_missing_values(df, name)\n",
    "    check_duplicate_keys(df, key, name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
