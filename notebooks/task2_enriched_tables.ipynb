{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d8689b6",
   "metadata": {},
   "source": [
    "# Task 2: Create Enriched Tables for Customers and Products\n",
    "\n",
    "This notebook creates enriched tables with calculated metrics, customer segmentation, and product performance analysis.\n",
    "\n",
    "## Objectives:\n",
    "- Enrich customer data with purchase behavior and segmentation\n",
    "- Enhance product data with sales performance and profitability metrics\n",
    "- Calculate derived metrics like Customer Lifetime Value, RFM scores\n",
    "- Implement business logic for customer and product classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "398d4515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Spark session initialized successfully\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add parent directory to path so we can import from src folder\n",
    "# When running from notebooks folder, go up one level to reach project root\n",
    "current_dir = os.getcwd()\n",
    "if 'notebooks' in current_dir:\n",
    "    parent_dir = os.path.dirname(current_dir)\n",
    "else:\n",
    "    parent_dir = current_dir\n",
    "    \n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.insert(0, parent_dir)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col, sum as spark_sum, count, avg, max as spark_max, when, lit, round as spark_round\n",
    "from src.processing import init_spark, get_customer_metrics, analyze_product_performance\n",
    "from src.config import BusinessConfig\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = init_spark(\"Task2_EnrichedTables\")\n",
    "print(\"✓ Spark session initialized successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec3ade87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loading Raw Data...\n",
      "✓ Raw data loaded successfully\n",
      "Customers: 5 rows\n",
      "Orders: 7 rows\n",
      "Products: 3 rows\n"
     ]
    }
   ],
   "source": [
    "# Load raw data (from Task 1 or sample data)\n",
    "print(\"✓ Loading Raw Data...\")\n",
    "\n",
    "# Sample data for demonstration\n",
    "customer_data = [\n",
    "    (1, \"John Doe\", \"USA\"),\n",
    "    (2, \"Jane Smith\", \"UK\"), \n",
    "    (3, \"Bob Wilson\", \"Canada\"),\n",
    "    (4, \"Alice Brown\", \"USA\"),\n",
    "    (5, \"Charlie Davis\", \"Germany\")\n",
    "]\n",
    "customer_schema = StructType([\n",
    "    StructField(\"Customer ID\", IntegerType(), False),\n",
    "    StructField(\"Customer Name\", StringType(), False),\n",
    "    StructField(\"Country\", StringType(), False)\n",
    "])\n",
    "customers_df = spark.createDataFrame(customer_data, customer_schema)\n",
    "\n",
    "# Sample orders data with varied amounts for segmentation\n",
    "orders_data = [\n",
    "    (1, 1, 1, \"2023-01-01\", 2, 8000.0, 1600.0),   # High value customer\n",
    "    (2, 1, 2, \"2023-01-15\", 1, 5000.0, 1000.0),   # High value customer\n",
    "    (3, 2, 1, \"2023-02-01\", 1, 3000.0, 600.0),    # Medium value customer\n",
    "    (4, 2, 3, \"2023-02-15\", 2, 3500.0, 700.0),    # Medium value customer\n",
    "    (5, 3, 2, \"2023-03-01\", 1, 1500.0, 300.0),    # Low value customer\n",
    "    (6, 4, 1, \"2023-03-15\", 3, 4500.0, 900.0),    # Medium value customer\n",
    "    (7, 5, 3, \"2023-04-01\", 1, 2000.0, 400.0),    # Low value customer\n",
    "]\n",
    "orders_schema = StructType([\n",
    "    StructField(\"Order ID\", IntegerType(), False),\n",
    "    StructField(\"Customer ID\", IntegerType(), False),\n",
    "    StructField(\"Product ID\", IntegerType(), False),\n",
    "    StructField(\"Order Date\", StringType(), False),\n",
    "    StructField(\"Quantity\", IntegerType(), False),\n",
    "    StructField(\"Sales\", DoubleType(), False),\n",
    "    StructField(\"Profit\", DoubleType(), False)\n",
    "])\n",
    "orders_df = spark.createDataFrame(orders_data, orders_schema)\n",
    "\n",
    "# Sample products data\n",
    "products_data = [\n",
    "    (1, \"Enterprise Laptop\", \"Technology\", \"Computers\"),\n",
    "    (2, \"Executive Chair\", \"Furniture\", \"Office Furniture\"),\n",
    "    (3, \"Business Phone\", \"Technology\", \"Mobile Devices\")\n",
    "]\n",
    "products_schema = StructType([\n",
    "    StructField(\"Product ID\", IntegerType(), False),\n",
    "    StructField(\"Product Name\", StringType(), False),\n",
    "    StructField(\"Category\", StringType(), False),\n",
    "    StructField(\"Sub-Category\", StringType(), False)\n",
    "])\n",
    "products_df = spark.createDataFrame(products_data, products_schema)\n",
    "\n",
    "print(\"✓ Raw data loaded successfully\")\n",
    "# Use len() on the data lists instead of .count() to avoid Python worker crashes\n",
    "print(f\"Customers: {len(customer_data)} rows\")\n",
    "print(f\"Orders: {len(orders_data)} rows\")\n",
    "print(f\"Products: {len(products_data)} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bca016b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Creating Enriched Customer Table...\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o255.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 1 times, most recent failure: Lost task 0.0 in stage 4.0 (TID 4) (192.168.1.34 executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:789)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.io.EOFException\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:397)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)\r\n\t... 24 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:789)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.io.EOFException\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:397)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)\r\n\t... 24 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m enriched_customers = get_customer_metrics(orders_df, customers_df)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Use .take() to safely check row count without triggering worker crash\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m sample_rows = \u001b[43menriched_customers\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✓ Enriched customers table created with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(sample_rows)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m+ customers\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m enriched_customers.printSchema()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pyspark\\sql\\dataframe.py:1401\u001b[39m, in \u001b[36mDataFrame.take\u001b[39m\u001b[34m(self, num)\u001b[39m\n\u001b[32m   1372\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtake\u001b[39m(\u001b[38;5;28mself\u001b[39m, num: \u001b[38;5;28mint\u001b[39m) -> List[Row]:\n\u001b[32m   1373\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Returns the first ``num`` rows as a :class:`list` of :class:`Row`.\u001b[39;00m\n\u001b[32m   1374\u001b[39m \n\u001b[32m   1375\u001b[39m \u001b[33;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1399\u001b[39m \u001b[33;03m    [Row(age=14, name='Tom'), Row(age=23, name='Alice')]\u001b[39;00m\n\u001b[32m   1400\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1401\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pyspark\\sql\\dataframe.py:1257\u001b[39m, in \u001b[36mDataFrame.collect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1237\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[32m   1238\u001b[39m \n\u001b[32m   1239\u001b[39m \u001b[33;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m \u001b[33;03m[Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\u001b[39;00m\n\u001b[32m   1255\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1256\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m._sc):\n\u001b[32m-> \u001b[39m\u001b[32m1257\u001b[39m     sock_info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1258\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\py4j\\java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdeco\u001b[39m(*a: Any, **kw: Any) -> Any:\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    181\u001b[39m         converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\py4j\\protocol.py:326\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    324\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    327\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    328\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    331\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    332\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o255.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 1 times, most recent failure: Lost task 0.0 in stage 4.0 (TID 4) (192.168.1.34 executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:789)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.io.EOFException\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:397)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)\r\n\t... 24 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:789)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.io.EOFException\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:397)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)\r\n\t... 24 more\r\n"
     ]
    }
   ],
   "source": [
    "# Create Enriched Customer Table\n",
    "print(\"✓ Creating Enriched Customer Table...\")\n",
    "\n",
    "# Calculate customer metrics\n",
    "enriched_customers = get_customer_metrics(orders_df, customers_df)\n",
    "\n",
    "# Use .take() to safely check row count without triggering worker crash\n",
    "sample_rows = enriched_customers.take(10)\n",
    "print(f\"✓ Enriched customers table created with {len(sample_rows)}+ customers\")\n",
    "enriched_customers.printSchema()\n",
    "\n",
    "print(\"\\n✓ Enriched Customer Data (first 5 rows):\")\n",
    "for row in sample_rows[:5]:\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c700b582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customer Segmentation Analysis\n",
    "print(\"✓ Customer Segmentation Analysis...\")\n",
    "\n",
    "# Show customer segments\n",
    "segment_distribution = enriched_customers.groupBy(\"Customer Segment\").agg(\n",
    "    count(\"*\").alias(\"Customer Count\"),\n",
    "    spark_sum(\"Total Sales\").alias(\"Segment Total Sales\"),\n",
    "    avg(\"Total Sales\").alias(\"Avg Sales per Customer\")\n",
    ").orderBy(\"Segment Total Sales\", ascending=False)\n",
    "\n",
    "print(\"\\n✓ Customer Segment Distribution:\")\n",
    "seg_rows = segment_distribution.take(10)\n",
    "for row in seg_rows:\n",
    "    print(row)\n",
    "\n",
    "# High-value customer analysis\n",
    "print(\"\\n✓ High-Value Customer Analysis:\")\n",
    "high_value_customers = enriched_customers.filter(col(\"Customer Segment\") == \"High Value\")\n",
    "hv_rows = high_value_customers.select(\"Customer Name\", \"Country\", \"Total Sales\", \"Total Profit\", \"Total Orders\").take(10)\n",
    "print(f\"High-value customers: {len(hv_rows)}+ customers\")\n",
    "for row in hv_rows:\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7522bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Enriched Product Table\n",
    "print(\"✓ Creating Enriched Product Table...\")\n",
    "\n",
    "# Calculate product performance metrics\n",
    "enriched_products = analyze_product_performance(orders_df, products_df)\n",
    "\n",
    "# Use .take() to safely check row count\n",
    "product_rows = enriched_products.take(10)\n",
    "print(f\"✓ Enriched products table created with {len(product_rows)}+ products\")\n",
    "enriched_products.printSchema()\n",
    "\n",
    "print(\"\\n✓ Enriched Product Data (first 3 rows):\")\n",
    "for row in product_rows[:3]:\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a847833e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Product Performance Analysis\n",
    "print(\"✓ Product Performance Analysis...\")\n",
    "\n",
    "# Show performance by category\n",
    "category_performance = enriched_products.groupBy(\"Category\").agg(\n",
    "    count(\"*\").alias(\"Product Count\"),\n",
    "    spark_sum(\"Total Sales\").alias(\"Category Total Sales\"),\n",
    "    avg(\"Profit Margin\").alias(\"Avg Profit Margin\"),\n",
    "    spark_sum(\"Total Profit\").alias(\"Category Total Profit\")\n",
    ").orderBy(\"Category Total Sales\", ascending=False)\n",
    "\n",
    "print(\"\\n✓ Category Performance:\")\n",
    "cat_rows = category_performance.take(10)\n",
    "for row in cat_rows:\n",
    "    print(row)\n",
    "\n",
    "# Top performing products\n",
    "print(\"\\n✓ Top Performing Products (by Sales):\")\n",
    "top_products = enriched_products.orderBy(col(\"Total Sales\").desc())\n",
    "top_rows = top_products.select(\"Product Name\", \"Category\", \"Sub-Category\", \"Total Sales\", \"Profit Margin\", \"Performance Flag\").take(5)\n",
    "for row in top_rows:\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e7b221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Customer Analytics\n",
    "print(\"✓ Advanced Customer Analytics...\")\n",
    "\n",
    "# Customer Activity Status Analysis\n",
    "activity_distribution = enriched_customers.groupBy(\"Activity Status\").agg(\n",
    "    count(\"*\").alias(\"Customer Count\"),\n",
    "    avg(\"Total Sales\").alias(\"Avg Sales\"),\n",
    "    avg(\"Days Since Order\").alias(\"Avg Days Since Order\")\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Customer Activity Distribution:\")\n",
    "activity_rows = activity_distribution.take(10)\n",
    "for row in activity_rows:\n",
    "    print(row)\n",
    "\n",
    "# Customer Value vs Activity Cross-Analysis\n",
    "print(\"\\n✓ Customer Value vs Activity Analysis:\")\n",
    "value_activity_matrix = enriched_customers.groupBy(\"Customer Segment\", \"Activity Status\").agg(\n",
    "    count(\"*\").alias(\"Count\")\n",
    ").orderBy(\"Customer Segment\", \"Activity Status\")\n",
    "\n",
    "matrix_rows = value_activity_matrix.take(20)\n",
    "for row in matrix_rows:\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5731cbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Product Profitability Deep Dive\n",
    "print(\"✓ Product Profitability Analysis...\")\n",
    "\n",
    "# Profit margin distribution\n",
    "profit_margin_ranges = enriched_products.withColumn(\n",
    "    \"Margin Range\",\n",
    "    when(col(\"Profit Margin\") >= 25, \"Excellent (25%+)\")\n",
    "    .when(col(\"Profit Margin\") >= 15, \"Good (15-25%)\")\n",
    "    .when(col(\"Profit Margin\") >= 5, \"Fair (5-15%)\")\n",
    "    .otherwise(\"Poor (<5%)\")\n",
    ")\n",
    "\n",
    "margin_distribution = profit_margin_ranges.groupBy(\"Margin Range\").agg(\n",
    "    count(\"*\").alias(\"Product Count\"),\n",
    "    avg(\"Total Sales\").alias(\"Avg Sales\")\n",
    ").orderBy(\"Product Count\", ascending=False)\n",
    "\n",
    "print(\"\\n✓ Profit Margin Distribution:\")\n",
    "margin_rows = margin_distribution.take(10)\n",
    "for row in margin_rows:\n",
    "    print(row)\n",
    "\n",
    "# Category profitability comparison\n",
    "print(\"\\n✓ Category Profitability Comparison:\")\n",
    "category_profitability = enriched_products.groupBy(\"Category\", \"Sub-Category\").agg(\n",
    "    spark_sum(\"Total Sales\").alias(\"Total Sales\"),\n",
    "    spark_sum(\"Total Profit\").alias(\"Total Profit\"),\n",
    "    avg(\"Profit Margin\").alias(\"Avg Margin\")\n",
    ").orderBy(\"Total Profit\", ascending=False)\n",
    "\n",
    "prof_rows = category_profitability.take(10)\n",
    "for row in prof_rows:\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485039dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Quality Validation for Enriched Tables\n",
    "print(\"✓ Data Quality Validation...\")\n",
    "\n",
    "def validate_enriched_data():\n",
    "    \"\"\"Validate enriched tables data quality\"\"\"\n",
    "    print(\"\\n✓ Enriched Tables Validation:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Customer table validation\n",
    "    print(\"\\n✓ Customer Table Validation:\")\n",
    "    \n",
    "    # Check for null values in calculated fields - use .take() instead of .count()\n",
    "    null_total_sales = len(enriched_customers.filter(col(\"Total Sales\").isNull()).take(100))\n",
    "    null_segments = len(enriched_customers.filter(col(\"Customer Segment\").isNull()).take(100))\n",
    "    \n",
    "    print(f\"  Null Total Sales: {null_total_sales}\")\n",
    "    print(f\"  Null Customer Segments: {null_segments}\")\n",
    "    \n",
    "    # Validate segment logic - use .take() instead of .count()\n",
    "    high_value_sample = enriched_customers.filter(col(\"Customer Segment\") == \"High Value\").take(100)\n",
    "    medium_value_sample = enriched_customers.filter(col(\"Customer Segment\") == \"Medium Value\").take(100)\n",
    "    low_value_sample = enriched_customers.filter(col(\"Customer Segment\") == \"Low Value\").take(100)\n",
    "    \n",
    "    print(f\"  High Value: {len(high_value_sample)}+, Medium Value: {len(medium_value_sample)}+, Low Value: {len(low_value_sample)}+\")\n",
    "    \n",
    "    # Product table validation\n",
    "    print(\"\\n✓ Product Table Validation:\")\n",
    "    \n",
    "    null_profit_margins = len(enriched_products.filter(col(\"Profit Margin\").isNull()).take(100))\n",
    "    null_performance_flags = len(enriched_products.filter(col(\"Performance Flag\").isNull()).take(100))\n",
    "    \n",
    "    print(f\"  Null Profit Margins: {null_profit_margins}\")\n",
    "    print(f\"  Null Performance Flags: {null_performance_flags}\")\n",
    "    \n",
    "    # Validate profit margin calculations - use .take() instead of .count()\n",
    "    incorrect_margins_sample = enriched_products.filter(\n",
    "        (col(\"Total Sales\") > 0) & \n",
    "        (abs(col(\"Profit Margin\") - (col(\"Total Profit\") / col(\"Total Sales\") * 100)) > 0.1)\n",
    "    ).take(100)\n",
    "    \n",
    "    print(f\"  Incorrect Margin Calculations: {len(incorrect_margins_sample)}\")\n",
    "    \n",
    "    if null_total_sales == 0 and null_segments == 0 and null_profit_margins == 0 and len(incorrect_margins_sample) == 0:\n",
    "        print(\"\\n✓ All validation checks passed!\")\n",
    "    else:\n",
    "        print(\"\\n⚠ Some validation issues detected\")\n",
    "\n",
    "validate_enriched_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02709429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create views for SQL access\n",
    "print(\"✓ Creating Temporary Views...\")\n",
    "\n",
    "enriched_customers.createOrReplaceTempView(\"enriched_customers\")\n",
    "enriched_products.createOrReplaceTempView(\"enriched_products\")\n",
    "\n",
    "print(\"✓ Views created:\")\n",
    "print(\"  - enriched_customers\")\n",
    "print(\"  - enriched_products\")\n",
    "\n",
    "# Test SQL queries - use LIMIT to avoid worker crashes\n",
    "print(\"\\n✓ Testing SQL Access:\")\n",
    "\n",
    "print(\"\\nCustomer Segment Summary:\")\n",
    "query1_result = spark.sql(\"\"\"\n",
    "    SELECT Customer_Segment, \n",
    "           COUNT(*) as count, \n",
    "           ROUND(AVG(Total_Sales), 2) as avg_sales\n",
    "    FROM enriched_customers \n",
    "    GROUP BY Customer_Segment\n",
    "    ORDER BY avg_sales DESC\n",
    "    LIMIT 10\n",
    "\"\"\").take(10)\n",
    "for row in query1_result:\n",
    "    print(row)\n",
    "\n",
    "print(\"\\nProduct Performance Summary:\")\n",
    "query2_result = spark.sql(\"\"\"\n",
    "    SELECT Performance_Flag, \n",
    "           COUNT(*) as count, \n",
    "           ROUND(AVG(Profit_Margin), 2) as avg_margin\n",
    "    FROM enriched_products \n",
    "    GROUP BY Performance_Flag\n",
    "    ORDER BY avg_margin DESC\n",
    "    LIMIT 10\n",
    "\"\"\").take(10)\n",
    "for row in query2_result:\n",
    "    print(row)\n",
    "\n",
    "print(\"\\n✓ Task 2 completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e982c948",
   "metadata": {},
   "source": [
    "## Summary of Task 2: Enriched Tables Creation\n",
    "\n",
    "###  Accomplished:\n",
    "1. **Customer Enrichment**: Created enriched customer table with metrics and segmentation\n",
    "2. **Product Enrichment**: Enhanced product data with performance analytics\n",
    "3. **Business Logic**: Implemented customer segmentation and product classification\n",
    "4. **Advanced Analytics**: Customer lifetime value, activity status, and profitability analysis\n",
    "\n",
    "###  Enriched Tables Created:\n",
    "\n",
    "#### Customer Enrichment:\n",
    "- **Total Sales & Profit**: Aggregated customer purchase history\n",
    "- **Customer Segmentation**: High/Medium/Low value classification\n",
    "- **Activity Status**: Active/Inactive customer classification\n",
    "- **Order Metrics**: Total orders, average order value, days since last order\n",
    "\n",
    "#### Product Enrichment:\n",
    "- **Sales Performance**: Total sales, profit, and quantity metrics\n",
    "- **Profitability**: Profit margin calculations and classifications\n",
    "- **Performance Flags**: High/Good/Needs Improvement categorization\n",
    "- **Category Analytics**: Performance by product categories\n",
    "\n",
    "###  Key Insights:\n",
    "- Customer segmentation reveals value distribution across customer base\n",
    "- Product performance analysis identifies top performers and improvement opportunities\n",
    "- Activity status helps identify at-risk customers for retention strategies\n",
    "- Profit margin analysis guides pricing and product mix decisions\n",
    "\n",
    "###  Next Steps:\n",
    "Ready for Task 3: Create enriched orders table with complete customer and product information."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
